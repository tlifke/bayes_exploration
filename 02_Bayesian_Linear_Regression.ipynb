{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Statistics: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear regression problem is defined as:\n",
    "\n",
    "y_i = w_i * x_i + E_i\n",
    "\n",
    "Where:\n",
    "* y_i is the vector of outcomes at step i\n",
    "* w_i is the vector of weights (coefficients) at step i\n",
    "* x_i is the vector of features (input data) at step i\n",
    "* E_i is the error at step i\n",
    "\n",
    "We must define a likelihood function, a prior distribution, and a posterior distribution to get our predictive distribution from which we can generate our linear regression.\n",
    "\n",
    "Likelihood:\n",
    "* Given the current model, how likely is the data pair y_i,x_i?\n",
    "* p(y_i|x_i,w_i) = N(w_i * x_i.T,B^-1)\n",
    "* w_i * x_i.T provides the vector mean estimate of y_i\n",
    "* B^-1 is the sigma^2 or noise around that mean estimate\n",
    "* This combination gives us a likelihood distribution of Normal/Gaussian shape.\n",
    "\n",
    "Prior:\n",
    "* For a Gaussian likelihood distribution, the appropriate prior is also a Gaussian:\n",
    "    * p(w_0) = n(m_0,S_0)\n",
    "* Our initial prior should look like:\n",
    "    * m_0 = 0 in a n length vector\n",
    "    * S_0 = alpha^-1 in a n by n covariance matrix\n",
    "* This is simply the definition of a prior for a Gaussian likelihood distribution. Note that we could use a uniform distribution over some range near 0 as well as that could be considered an uninformed prior; however, we use a Gaussian because it makes an analytical solution simpler to calculate as our prior updates according to a Gaussian update step.\n",
    "\n",
    "Posterior:\n",
    "* The updated prior, given the new data pair y_i, x_i.\n",
    "* p(w_i+1|w_i,x_i,y_i) = N(m_i+1,S_i+1) where:\n",
    "    * S_i+1 = (S_i^-1 + B * x_i.T * x_i)^-1\n",
    "    * m_i+1 = S_i+1 * (S_i^-1* m_i + B * x_i * y_i)\n",
    "\n",
    "Predictive Distribution:\n",
    "* p(y_i) = N(u_i,sigma_i) where\n",
    "    * u_i = w_i * x_i.T\n",
    "    * sigma_i = 1/B * x_i * S_i * x_i.T\n",
    "* Thus, our line of best fit is u_i (our MAP) and we can expect some variance sigma_i around it, informed by the error of our line.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Bayesian Linear Regression class using the above definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesLinReg:\n",
    "    \"\"\"\n",
    "    State\n",
    "        self.n_features: the number of features \n",
    "        self.alpha: the shape parameter for the inverse gamma distribution\n",
    "        self.beta: the scale parameter for the inverse gamma distribution\n",
    "        self.mean: the coefficients currently estimating the line/\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,n_features,alpha,beta):\n",
    "        \"\"\"\n",
    "        Initialize the state variables\n",
    "        \"\"\"\n",
    "        self.n_features = n_features\n",
    "        self.alpha = alpha\n",
    "        self.beta  = beta\n",
    "        self.mean = np.zeros(n_features)\n",
    "        self.cov_inv = np.identity(n_features) / alpha\n",
    "    \n",
    "    def fit_step(self,x,y):\n",
    "        \"\"\"\n",
    "        Perform a single step in training the bayesian regression\n",
    "        \"\"\"\n",
    "        \n",
    "        # Update the inverse covariance matrix\n",
    "        cov_inv = self.cov_inv + self.beta * np.outer(x, x)\n",
    "\n",
    "        # Update the mean vector using the new covariance matrix \n",
    "        cov = np.linalg.inv(cov_inv)\n",
    "        mean = cov @ (self.cov_inv @ self.mean + self.beta * y * x)\n",
    "\n",
    "        # Finish the update\n",
    "        self.cov_inv = cov_inv\n",
    "        self.mean = mean\n",
    "        \n",
    "        return self   \n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Using the current model, estimate the y of the given x.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Obtain the predictive mean\n",
    "        # Note that this predictive mean is not our draw.\n",
    "        y_pred_mean = x @ self.mean\n",
    "\n",
    "        # Obtain the predictive variance\n",
    "        \n",
    "        w_cov = np.linalg.inv(self.cov_inv)\n",
    "        y_pred_var = 1 / self.beta + x @ w_cov @ x.T\n",
    "\n",
    "        # We use the covariance (divided by two for standard deviation)\n",
    "        # and mean to generate the distribution, then return the distribution\n",
    "        return stats.norm(loc=y_pred_mean, scale=y_pred_var ** .5)\n",
    "    \n",
    "    @property\n",
    "    def weights_dist(self):\n",
    "        \"\"\"\n",
    "        Return the distribution of the weights\n",
    "        \"\"\"\n",
    "        \n",
    "        cov = np.linalg.inv(self.cov_inv)\n",
    "        return stats.multivariate_normal(mean=self.mean, cov=cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7888798406328736\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "\n",
    "X, y = datasets.load_boston(return_X_y=True)\n",
    "\n",
    "model = BayesLinReg(n_features=X.shape[1], alpha=1, beta=1)\n",
    "\n",
    "y_pred = np.empty(len(y))\n",
    "\n",
    "for i, (xi, yi) in enumerate(zip(X, y)):\n",
    "    y_pred[i] = model.predict(xi).mean()\n",
    "    model.fit_step(xi, yi)\n",
    "\n",
    "print(metrics.mean_absolute_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Pick some true parameters that the model has to find\n",
    "weights = np.array([-.3, .5])\n",
    "\n",
    "def sample(n):\n",
    "    for _ in range(n):\n",
    "        x = np.array([1, np.random.uniform(-1, 1)])\n",
    "        y = np.dot(weights, x) + np.random.normal(0, .2)\n",
    "        yield x, y\n",
    "\n",
    "model = BayesLinReg(n_features=2, alpha=2, beta=25)\n",
    "\n",
    "# The following 3 variables are just here for plotting purposes\n",
    "N = 100\n",
    "w = np.linspace(-1, 1, 100)\n",
    "W = np.dstack(np.meshgrid(w, w))\n",
    "\n",
    "n_samples = 10\n",
    "fig = plt.figure(figsize=(7 * n_samples, 21))\n",
    "grid = ImageGrid(\n",
    "    fig, 111,  # similar to subplot(111)\n",
    "    nrows_ncols=(n_samples, 3),  # creates a n_samplesx3 grid of axes\n",
    "    axes_pad=.5  # pad between axes in inch.\n",
    ")\n",
    "\n",
    "# We'll store the features and targets for plotting purposes\n",
    "xs = []\n",
    "ys = []\n",
    "\n",
    "def prettify_ax(ax):\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.set_ylim(-1, 1)\n",
    "    ax.set_xlabel('$w_1$')\n",
    "    ax.set_ylabel('$w_2$')\n",
    "    return ax\n",
    "\n",
    "for i, (xi, yi) in enumerate(sample(n_samples)):\n",
    "\n",
    "    pred_dist = model.predict(xi)\n",
    "\n",
    "    # Prior weight distribution\n",
    "    ax = prettify_ax(grid[3 * i])\n",
    "    ax.set_title(f'Prior weight distribution #{i + 1}')\n",
    "    ax.contourf(w, w, model.weights_dist.pdf(W), N, cmap='viridis')\n",
    "    ax.scatter(*weights, color='red')  # true weights the model has to find\n",
    "\n",
    "    # Update model\n",
    "    model.fit_step(xi, yi)\n",
    "\n",
    "    # Prior weight distribution\n",
    "    ax = prettify_ax(grid[3 * i + 1])\n",
    "    ax.set_title(f'Posterior weight distribution #{i + 1}')\n",
    "    ax.contourf(w, w, model.weights_dist.pdf(W), N, cmap='viridis')\n",
    "    ax.scatter(*weights, color='red')  # true weights the model has to find\n",
    "\n",
    "    # Posterior target distribution\n",
    "    xs.append(xi)\n",
    "    ys.append(yi)\n",
    "    posteriors = [model.predict(np.array([1, wi])) for wi in w]\n",
    "    ax = prettify_ax(grid[3 * i + 2])\n",
    "    ax.set_title(f'Posterior target distribution #{i + 1}')\n",
    "    # Plot the old points and the new points\n",
    "    ax.scatter([xi[1] for xi in xs[:-1]], ys[:-1])\n",
    "    ax.scatter(xs[-1][1], ys[-1], marker='*')\n",
    "    # Plot the predictive mean along with the predictive interval\n",
    "    ax.plot(w, [p.mean() for p in posteriors], linestyle='--')\n",
    "    cis = [p.interval(.95) for p in posteriors]\n",
    "    ax.fill_between(\n",
    "        x=w,\n",
    "        y1=[ci[0] for ci in cis],\n",
    "        y2=[ci[1] for ci in cis],\n",
    "        alpha=.1\n",
    "    )\n",
    "    # Plot the true target distribution\n",
    "    ax.plot(w, [np.dot(weights, [1, xi]) for xi in w], color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
